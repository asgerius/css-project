{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit3146a4cc6db04886890e25bf876d3a33",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Network Analysis\n",
    "### The next section contains the network analysis of programmers interacting with other programmers on the basis of having a problem with a given programming language.\n",
    "#### The following steps are taken in this part of the Stack Overflow investigation:\n",
    "\n",
    "1. Create individual networks for each programming language with authors as nodes and interactions (answering questions or commenting answers) as links.\n",
    "\n",
    "2. Perform basic network analysis (counts, degree distribution etc.) on language-specific networks.\n",
    "\n",
    "3. Create one big StackOverflow-network including all 16 programming languages with same types of nodes and links.\n",
    "\n",
    "4. Perform basic network analysis on the StackOverflow-network.\n",
    "\n",
    "5. Vizualize the StackOverflow-network.\n",
    "\n",
    "6. Perform advanced network analysis on the StackOverflow-network by investigating the different subnetworks, communities, modularity etc.\n",
    "\n",
    "7. Use the Louvain algorithm to create a network from the data and compare to the StackOverflow-network.\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import netwulf as nf\n",
    "from scipy import stats \n",
    "from operator import itemgetter \n",
    "from collections import Counter\n",
    "import re\n",
    "from glob import glob as glob  # glob\n",
    "from tqdm import tqdm\n",
    "from pelutils import Table, thousand_seps\n",
    "import itertools\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def setup_mpl():\n",
    "    mpl.rcParams['font.family'] = \"Liberation Serif\"\n",
    "    mpl.rcParams['font.size'] = 11\n",
    "    mpl.rcParams['figure.figsize'] = (7,2.5)\n",
    "    mpl.rcParams['figure.dpi'] = 200\n",
    "    #mpl.rcParams['lines.linewidth'] = 1\n",
    "setup_mpl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loved_languages = {\n",
    "    \"rust\":        86.1,\n",
    "    \"typescript\":  67.1,\n",
    "    \"python\":      66.7,\n",
    "    \"kotlin\":      62.9,\n",
    "    \"go\":          62.3,\n",
    "    \"dart\":        62.1,\n",
    "    \"c#\":          59.7,\n",
    "    \"javascript\":  58.3,\n",
    "    \"haskell\":     51.7,\n",
    "    \"java\":        44.1,\n",
    "    \"c++\":         43.4,\n",
    "    \"ruby\":        42.9,\n",
    "    \"c\":           33.1,\n",
    "    \"perl\":        28.6,\n",
    "    \"objective-c\": 23.4,\n",
    "    \"vba\":         19.6,\n",
    "}\n",
    "data = pd.read_pickle('/home/augustsemrau/drive/6semester/CSS_02467/css-project/data_timesorted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = data#[:50000]\n",
    "# Data summary\n",
    "t = Table()\n",
    "t.add_row([\"Language\", \"Questions\", \"Answers\", \"Comments\", \"Total\"])\n",
    "for lang in tqdm(loved_languages):\n",
    "    t.add_row([\n",
    "        lang.capitalize(),\n",
    "        *[thousand_seps(sum((so[\"language\"] == lang) & (so[\"type\"] == t))) for t in (\"q\", \"a\", \"c\")],\n",
    "        thousand_seps(sum(so[\"language\"] == lang)),\n",
    "    ], [1, 0, 0, 0, 0])\n",
    "t.add_row([\n",
    "    \"\",\n",
    "    thousand_seps(sum(so[\"type\"] == \"q\")),\n",
    "    thousand_seps(sum(so[\"type\"] == \"a\")),\n",
    "    thousand_seps(sum(so[\"type\"] == \"c\")),\n",
    "    thousand_seps(len(so)),\n",
    "], [1, 0, 0, 0, 0])\n",
    "print(\"Number of items in dataset\")\n",
    "print(t)"
   ]
  },
  {
   "source": [
    "1. Create individual networks for each programming language with authors as nodes and interactions (answering questions or commenting answers) as links."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for creating programming-language-specific networks\n",
    "def lang_networks(data, prog_language):\n",
    "    lang_data = data.loc[data['language'] == prog_language]\n",
    "    print(lang_data.shape)\n",
    "    lang_questions = lang_data.loc[lang_data['type'] == 'q']\n",
    "    lang_answers = lang_data.loc[lang_data['type'] == 'a']\n",
    "    lang_comments = lang_data.loc[lang_data['type'] == 'c']\n",
    "\n",
    "    ## Get authors of questions and answers\n",
    "    question_authors = dict(zip(lang_questions['question_id'], lang_questions['owner/user_id']))\n",
    "    answer_question_authors = dict(zip(lang_answers['answer_id'], lang_answers['owner/user_id']))\n",
    "\n",
    "    ## Find parent id's of all datapoints\n",
    "    lang_data['parent_author'] = \"\"\n",
    "    for index, row in tqdm(lang_data.iterrows()):\n",
    "        if row['type'] == 'q':\n",
    "            row['parent_author'] = None\n",
    "        elif row['type'] == 'a':\n",
    "            try:\n",
    "                row['parent_author'] = question_authors[str(row['question_id'])]\n",
    "            except:\n",
    "                row['parent_author'] = None\n",
    "        else:\n",
    "            try:\n",
    "                row['parent_author'] = answer_authors[str(row['answer_id'])]\n",
    "            except:\n",
    "                row['parent_author'] = None\n",
    "\n",
    "    ## Filter for NANs, there are a lot for comments..\n",
    "    print(f'{language} data before filtering for NANs', lang_data.shape)\n",
    "    lang_data = lang_data[lang_data['parent_author'].notnull()]\n",
    "    lang_data = lang_data[lang_data['owner/user_id'].notnull()]\n",
    "    lang_data = lang_data[lang_data['parent_author'] != 'None']\n",
    "    lang_data = lang_data[lang_data['owner/user_id'] != 'None']\n",
    "    print(f'{language} data after filtering for NANs', lang_data.shape)\n",
    "\n",
    "    ## Create weighted edge list\n",
    "    edge_list = lang_data.groupby(['owner/user_id', 'parent_author']).size().to_frame('weight').reset_index()\n",
    "\n",
    "    ## Define nodes and weights\n",
    "    sources = list(edge_list['owner/user_id'])\n",
    "    targets = list(edge_list['parent_author'])\n",
    "    weights = list(edge_list['weight'])\n",
    "    \n",
    "    ## Create tuple list of edges with weights\n",
    "    edges = [(sources[i], targets[i], weights[i]) for i in range(len(sources))]\n",
    "\n",
    "    ## Create graph and add nodes and edges\n",
    "    lang_graph = nx.DiGraph()\n",
    "    lang_graph.add_nodes_from(sources)\n",
    "    lang_graph.add_nodes_from(targets)\n",
    "    lang_graph.add_weighted_edges_from(edges)\n",
    "\n",
    "    return lang_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell creates the graphs for each programming language and saves a pickle\n",
    "for prog_lang in graphs.keys():\n",
    "    print(prog_lang)\n",
    "    prog_lang_graph = lang_networks(data=data, prog_language=prog_lang)\n",
    "    ## Save to pickle for later use\n",
    "    path = '/home/augustsemrau/drive/6semester/CSS_02467/css-project/data/graphs/' + str(prog_lang) + '_graph.pkl'\n",
    "    nx.write_gpickle(prog_lang_graph, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell loads all the pickled graphs\n",
    "graphs = {prog_lang : None for prog_lang in loved_languages.keys()}\n",
    "\n",
    "for prog_lang in graphs.keys():\n",
    "    print(prog_lang)\n",
    "    prog_lang_graph_path = '/home/augustsemrau/drive/6semester/CSS_02467/css-project/data/graphs/' + str(prog_lang) + '_graph.pkl'\n",
    "    graphs[prog_lang] = nx.read_gpickle(prog_lang_graph_path)"
   ]
  },
  {
   "source": [
    "2. Perform basic network analysis (counts, degree distribution etc.) on language-specific networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for printing basic analysis\n",
    "def basic_graph_analysis(graph, language):\n",
    "\n",
    "    ## Number of nodes, links and density\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_links = graph.number_of_edges()\n",
    "    density = nx.classes.function.density(graph)\n",
    "\n",
    "    print(f\"The number of nodes in the {language} graph: \", num_nodes)\n",
    "    print(f\"The number of links in the {language} graph: \", num_links)\n",
    "    print(f\"The density of the {language} graph: \", density)\n",
    "\n",
    "    ## Average, median, mode, minimum and maximum value of the in and out-degrees\n",
    "    in_degrees_dict = dict(graph.in_degree())\n",
    "    out_degrees_dict = dict(graph.out_degree())\n",
    "\n",
    "    in_degrees = list(in_degrees_dict.values())\n",
    "    out_degrees = list(out_degrees_dict.values())\n",
    "\n",
    "    print(f\"In-degree of {language} users in the graph: \")\n",
    "    print(\"  Average:\", np.mean(in_degrees))\n",
    "    print(\"  Median: \", np.median(in_degrees))\n",
    "    print(\"  Mode:    {0} with {1} occurences.\".format(int(stats.mode(in_degrees)[0]), int(stats.mode(in_degrees)[1])))\n",
    "    print(\"  Minimum:\", min(in_degrees))\n",
    "    print(\"  Maximum:\", max(in_degrees))\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"Out-degree of {language} users in the graph: \")\n",
    "    print(\"  Average:\", np.mean(out_degrees))\n",
    "    print(\"  Median: \", np.median(out_degrees))\n",
    "    print(\"  Mode:    {0} with {1} occurences.\".format(int(stats.mode(out_degrees)[0]), int(stats.mode(out_degrees)[1])))\n",
    "    print(\"  Minimum:\", min(out_degrees))\n",
    "    print(\"  Maximum:\", max(out_degrees))\n",
    "\n",
    "    ## Plot of the distribution of in-degrees and out-degrees, using a logarithmic binning\n",
    "\n",
    "    # Compute histogram\n",
    "    bins = np.logspace(0, np.log10(max(in_degrees)), 15)\n",
    "    hist, edges = np.histogram(in_degrees, bins=bins, density=True)\n",
    "    x_in = (edges[1:] + edges[:-1]) / 2.\n",
    "\n",
    "    bins = np.logspace(0, np.log10(max(out_degrees)), 15)\n",
    "    hist, edges = np.histogram(out_degrees, bins=bins, density=True)\n",
    "    x_out = (edges[1:] + edges[:-1]) / 2.\n",
    "\n",
    "    # Both in and out-degrees plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_in, hist, marker='.', label='In-Degrees')\n",
    "    ax.plot(x_out, hist, marker='.', label='Out-Degrees')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Number of degrees')\n",
    "    ax.set_ylabel('Probability Density')\n",
    "    ax.set_title(f\"Logarithmically binned distribution plot of the {language}-graph\")\n",
    "    ax.grid()\n",
    "    ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_graph_analysis(haskell_graph, 'haskell')"
   ]
  },
  {
   "source": [
    "3. Create one big StackOverflow-network including all 16 programming languages with same types of nodes and links."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "4. Perform basic network analysis on the StackOverflow-network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "5. Vizualize the StackOverflow-network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.visualize(so_graph)"
   ]
  },
  {
   "source": [
    "6. Perform advanced network analysis on the StackOverflow-network by investigating the different subnetworks, communities, modularity etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "7. Use the Louvain algorithm to create a network from the data and compare to the StackOverflow-network.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}