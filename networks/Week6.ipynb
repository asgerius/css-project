{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some important admin and homework for next time.\n",
    "\n",
    "\n",
    "1. __Important point number 1__. We are reaching the mid-term of the semester and it is time for some reflection :) I want to know from you guys how the class is going. This is super important for me to make sure that the second half of the semester goes well. Before working on the material for today, please __fill [the mid-term survery ](https://docs.google.com/forms/d/e/1FAIpQLSc79NAmXoPfYjmVyVgCDUdyK8narAOuR-CmgYhQM6h3vRZfuA/viewform?usp=sf_link)__ on Google Forms. I promise it should not take more than 5 minutes. \n",
    "\n",
    "\n",
    "2. __Important point number 2__. We are slowly approaching the time when you guys will work on your own project. There are still a couple of classes to go, but by now you have a good idea of the methods and types of questions we work with in this class. And I would like to start discussing your ideas. Make sure you __complete Exercise 5 (at the end of this notebook) before next Wednesday__. We will take some time to talk about it next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of today's class.\n",
    "\n",
    "This week's curriculum is about text analysis. The overview is\n",
    "\n",
    "* Tricks for raw text (NLPP Chapter 3) and finding the important words in a document (TF-IDF)\n",
    "* Apply these tricks to study the content of submissions \n",
    "\n",
    "In the first part, we will take a quick tour of NLPP's chapter 3, which is boring, but an amazing resource that you'll keep returning to. Then we'll talk about how we can use simple statistics to get text to show us what it's all about. We will even do a little visualization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Processing real text (from out on the inter-webs)\n",
    "\n",
    "Ok. So Chapter 3 in NLPP is all about working with text from the real world. Getting text from this internet, cleaning it, tokenizing, modifying (e.g. stemming, converting to lower case, etc) to get the text in shape to work with the NLTK tools you've already learned about - and many more. In the process we'll learn more about regular expressions, as well as unicode; something we've already been struggling with a little bit will now be explained in more detail. \n",
    "> \n",
    "> **Video lecture**: Short overview of chapter 3 + a few words about kinds of language processing that we don't address in this class. \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"Rwakh-HXPJk\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Reading*: NLPP Chapter 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.9, and 3.10\\. It's not important that you go in depth with everything here - the key thing is that you *know that Chapter 3 of this book exists*, and that it's a great place to return to if you're ever in need of an explanation of regular expressions, unicode, or other topics that you forget as soon as you stop using them (and don't worry, I forget about those things too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise_ 1: Just a couple of examples from the book: Work through the exercises NLPP 3.12: 6, 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[a-zA-Z]+: A string that contains at least one letter, either lower or upper case.\n[A-Z][a-z]*: A string with zero or more letters in the alphabet.\np[aeiou]{,2}t: \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "re_show() missing 1 required positional argument: 'string'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92dc0a7b8fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p[aeiou]{,2}t: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[a-zA-Z]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: re_show() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "\"\"\" NLPP 3.12: 6:\n",
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+\n",
    "    [A-Z][a-z]*\n",
    "    p[aeiou]{,2}t\n",
    "    \\d+(\\.\\d+)?\n",
    "    ([^aeiou][aeiou][^aeiou])*\n",
    "    \\w+|[^\\w\\s]+\n",
    "\n",
    "Test your answers using nltk.re_show().\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Operator \tBehavior\n",
    ". \tWildcard, matches any character\n",
    "^abc \tMatches some pattern abc at the start of a string\n",
    "abc$ \tMatches some pattern abc at the end of a string\n",
    "[abc] \tMatches one of a set of characters\n",
    "[A-Z0-9] \tMatches one of a range of characters\n",
    "ed|ing|s \tMatches one of the specified strings (disjunction)\n",
    "* \tZero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
    "+ \tOne or more of previous item, e.g. a+, [a-z]+\n",
    "? \tZero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
    "{n} \tExactly n repeats where n is a non-negative integer\n",
    "{n,} \tAt least n repeats\n",
    "{,n} \tNo more than n repeats\n",
    "{m,n} \tAt least m and no more than n repeats\n",
    "a(b|c)+ \tParentheses that indicate the scope of the operators\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"[a-zA-Z]+: A string that contains at least one letter, either lower or upper case.\")\n",
    "print(\"[A-Z][a-z]*: A string with zero or more letters in the alphabet.\")\n",
    "print(\"p[aeiou]{,2}t: \")\n",
    "\n",
    "nltk.re_show(\"[a-zA-Z]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\nthough), 'I won't have any pepper in my kitchen AT ALL. Soup does very\nwell without--Maybe it's always pepper that makes people hot-tempered,'...\n'when i'm a duchess,' she said to herself, (not in a veri hope tone\nthough), 'I won't have ani pepper in my kitchen AT all. soup doe very\nwel without--mayb it' alway pepper that make peopl hot-tempered,'...\n'when i'm a duchess,' she said to herself, (not in a very hop tone\nthough), 'i won't hav any pep in my kitch at all. soup doe very\nwell without--maybe it's alway pep that mak peopl hot-tempered,'...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NLPP 3.12: 30:\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.\"\"\"\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokens = [\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
    "'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
    "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',\n",
    "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
    "\n",
    "print(' '.join(tokens))\n",
    "\n",
    "tokens_porter = [porter.stem(w) for w in tokens]\n",
    "tokens_lancaster = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "print(' '.join(tokens_porter))\n",
    "print(' '.join(tokens_lancaster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude to part 2. -  Data.\n",
    "\n",
    "In the follwing exercises, we will study the text contained in _r/wallstreebets_ submissions. To make things a bit more exciting, we will work with \\**all** the submissions posted in 2020 in _r/wallstreebets_. As you may well guess, we will need both the title and the content of each submission.\n",
    "\n",
    "To make things a bit less tedious for you guys, I downloaded and made avaialble the data you need (you can find it [here](https://github.com/lalessan/comsocsci2021/blob/master/data/wallstreet_subs.csv.gz)). The dataset consists of all the submissions posted between January 1st and December 31st 2020 with content in English. For each submission, you have the following information: timestamp of creation (__created_utc__), __title__, textual content (__selftext__), and __score__. You are welcome to use this data. If you prefer to download your own (see optional exercise below), that's even better!! As usual, I do not expect you to find a perfect match between your data and mine. In the exercises below, I refer to this data as the \"_wallstreetbets submissions dataset_\".\n",
    "\n",
    "_Exercise (Optional)_: \n",
    "\n",
    "> * Download all submissions posted on _r/wallstreetbets_ in 2020 using [psaw](https://pypi.org/project/psaw/).\n",
    "> * For each submission, keep the title, the textual content, the score, the author, and the time of creation. \n",
    "> * Remove submissions whose content has been removed, and those that are not in English. You can use the library [langdetect](https://pypi.org/project/langdetect/) to detect the language of a given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Words that characterize stocks discussed on r/wallstreetbets\n",
    "\n",
    "In this section, we'll begin to play around with how far we can get with simple strategies for looking at text. The video is Sune talking about a fun paper, which shows you how little is needed in order to reveal something very interesting about humans that produce text. Then, we'll use a very simple weighting scheme called TF-IDF to find the words in the reddit r/wallstreetbets submissions that charachterize different stocks. In cleaning the Reddit submissions, we'll use some of the stuff you've just read about above. Finally, we'll even visualize them in a fun little word cloud (below is what I found for the discussions around Gamestop, Microsoft, and Tesla). The wordclouds may not be immediately understandable. But if you do some research on the important words, you will find that the TF-IDF method extracts quite interesting information.\n",
    "\n",
    "<img src=\"https://github.com/lalessan/comsocsci2021/blob/master/files/wordclouds.png?raw=true\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Video lecture**: Simple methods reveal a lot. Sune talks a little bit about the paper: [Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"wkYvdfkVmlI\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 2: Most discussed stocks in r/wallstreebets_. GME is only one among many stocks people have discussed in _r/wallstreetbets_. In this exercise, we will find the most discussed stocks on _wallstreetbets_. Stocks are identified by their [Ticker symbol](https://en.wikipedia.org/wiki/Ticker_symbol). A Ticker symbol is nothing but a string consisting of letters and numbers, and is typically quite short. For example the Gamestop symbol is _GME_, Amazon is _AMZN_, Alphabet is _GOOGL_...\n",
    "\n",
    "> 1. To talk about a specific stock, Redditors often use the corresponding ticker symbol [preceded by the dollar sign](https://www.reddit.com/r/wallstreetbets/comments/5yvvue/why_do_you_put_a_dollar_sign_in_front_of_a_ticker/) (\\\\$GME, \\$AMZN...). Write down a [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression) matching words that begin with a dollar sign \"\\\\$\". See [NLPP book, section 3.4]().\n",
    "> 2. Load the _wallstreetbets submission dataset_ as a Pandas DataFrame and create a new column containing both the title and the textual content of each submission (as one long string). We refer to this as the _text_ of the submission.\n",
    "> 3. For each submission, find all ticker symbols (those preceded by a dollar sign) contained in the _text_. Use the function [re.findall](https://docs.python.org/3/library/re.html), and the regular expression you created in point 1). Some tips for success: \n",
    "> > * Remove matches that are definetly not stock symbols (for example amounts like: \\\\$100, \\$1000k).\n",
    "> > * Convert all matches to uppercase\n",
    "> > * Remove the dollar sign at the beginning of the symbol (e.g. \\\\$gme → GME).\n",
    "> 4. Create a list containing the top 15 Ticker Symbols by number of occurrences. GME should be among them. If it is not, check again your analysis and/or come talk to me. Google the top 15 symbols and find the corresponding company names. Are they known companies or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_subs = pd.read_csv('/home/augustsemrau/drive/6semester/CSS_02467/comsocsci2021/lectures/data/week6/wallstreet_subs.csv', parse_dates=['created_utc']).set_index('created_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. To talk about a specific stock, Redditors often use the corresponding ticker symbol [preceded by the dollar sign](https://www.reddit.com/r/wallstreetbets/comments/5yvvue/why_do_you_put_a_dollar_sign_in_front_of_a_ticker/) (\\$GME, \\$AMZN...). Write down a [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression) matching words that begin with a dollar sign \"\\$\". See [NLPP book, section 3.4]().\"\"\"\n",
    "\n",
    "# A stock ticker would be [$][a-zA-Z]{3,5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. Load the _wallstreetbets submission dataset_ as a Pandas DataFrame and create a new column containing both the title and the textual content of each submission (as one long string). We refer to this as the _text_ of the submission.\"\"\"\n",
    "\n",
    "wsb_subs['text'] = wsb_subs['title'] + ' ' + wsb_subs['selftext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"3. For each submission, find all ticker symbols (those preceded by a dollar sign) contained in the _text_. Use the function [re.findall](https://docs.python.org/3/library/re.html), and the regular expression you created in point 1). Some tips for success: \n",
    "> > * Remove matches that are definetly not stock symbols (for example amounts like: \\\\$100, \\$1000k).\n",
    "> > * Convert all matches to uppercase\n",
    "> > * Remove the dollar sign at the beginning of the symbol (e.g. \\\\$gme → GME).\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def find_ticker(string):\n",
    "    ticker = re.findall(r'[$][A-Za-z]{3,5}', string)\n",
    "    # if ticker != []:\n",
    "    #     print(ticker)\n",
    "    ticker = [x.upper()[1:] for x in ticker]\n",
    "    return ticker\n",
    "\n",
    "wsb_subs['tickers'] = wsb_subs.text.apply(find_ticker)\n",
    "# print(wsb_subs.iloc[-1])\n",
    "wsb_subs.to_csv('/home/augustsemrau/drive/6semester/CSS_02467/comsocsci2021/lectures/data/week6/wsb_subs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"4. Create a list containing the top 15 Ticker Symbols by number of occurrences. GME should be among them. If it is not, check again your analysis and/or come talk to me. Google the top 15 symbols and find the corresponding company names. Are they known companies or not?\"\"\"\n",
    "tickers_all = wsb_subs.tickers.sum()\n",
    "\n",
    "tickers_freq = {}\n",
    "for ticker in tickers_all:\n",
    "    if ticker in tickers_freq:\n",
    "        tickers_freq[ticker] += 1\n",
    "    else:\n",
    "        tickers_freq[ticker] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'SPY': 1150, 'TSLA': 988, 'SPCE': 562, 'PLTR': 394, 'MSFT': 377, 'ROPE': 359, 'AAPL': 243, 'AMZN': 230, 'NIO': 228, 'AMD': 212, 'BABA': 207, 'GME': 202, 'DIS': 169, 'WORK': 149, 'VALE': 139}\n{'spy': 1150, 'tsla': 988, 'spce': 562, 'pltr': 394, 'msft': 377, 'rope': 359, 'aapl': 243, 'amzn': 230, 'nio': 228, 'amd': 212, 'baba': 207, 'gme': 202, 'dis': 169, 'work': 149, 'vale': 139, 'Other': []}\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter \n",
    "tickers_top15 = dict(sorted(tickers_freq.items(), key = itemgetter(1), reverse = True)[:15]) \n",
    "#sorted(tickers_freq.items(), key=lambda x:x[1])\n",
    "print(tickers_top15)\n",
    "top_15_stocks = {ticker.lower():count for ticker,count in tickers_top15.items()}\n",
    "top_15_stocks['Other'] = []\n",
    "print(top_15_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 3: TF-IDF and the stocks discussed on r/wallstreetbets._ The goal for this exercise is to find the words charachterizing each of the stocks discussed on r/wallstreetbets. We will focus on the top 15 stocks we idenfied in Exercise 2, and we will of course use TF-IDF.\n",
    "\n",
    " \n",
    "> 1. First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF. \n",
    ">   * What does TF stand for? \n",
    ">   * What does IDF stand for?\n",
    ">\n",
    "> 2. Tokenize the __text__ of each submission. Create a column __tokens__ in your dataframe containing the tokens. Remember the bullets below for success.\n",
    ">   * If you dont' know what _tokenization_ means, go back and read Chapter 3 again. **The advice to go back and check Chapter 3 is valid for every cleaning step below**.\n",
    ">   * Exclude punctuation.\n",
    ">   * Exclude URLs\n",
    ">   * Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    ">   * Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    ">   * Set everything to lower case.\n",
    ">   * **Note** that none of the above has to be perfect. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some submissions repeat certain words again and again and again, whereas other are very short. For that reason, I decided to use the unique set of words from each submission rather than each word in proportion to how it's actually used. Choices like that are up to you.\n",
    ">\n",
    "> 3. Find submissions discussing at least one of the top 15 stocks you identified above. To do so: \n",
    "> > * Create a function that finds the intersection between a list of tokens and your list of top 15 stocks. For example, your function applied to the tokens: _\"[Here, TSLA, submission, GME]\"_ should return [\"TSLA\",\"GME\"]. (_Optional_: you can also try to included cases in which the list of tokens contains a company name among your top 15. For example the function applied to _\"[Here, Gamestop, submission]\"_ could return ['GME'].)\n",
    "> > * Create a new column _stock_ in your DataFrame, containing the output of your function applied to the _text_ column. Values in this column should be lists. \n",
    "> > * Handle cases where one post discusses more than one stock by applying the function [__explode__](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) to the _stock_ column. This will duplicate submissions associated to multiple stocks. After exploding, the values included in the _stock_ column should be strings. \n",
    "> > * Handle cases where none of the selected stocks is discussed by replacing Nan values, for example with \"Other\".\n",
    ">\n",
    "> 4. Now, we want to find out which words are important for each *stock*, so we're going to create several ***large documents, one for each stock***. Each document includes all the tokens related to the same stock. We will also have a document including discussions that do not relate to the top 15 stocks.\n",
    "> 5. Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within __5 stocks of your choice__. \n",
    ">   * Describe similarities and differences between the stocks.\n",
    ">   * Why aren't the TFs not necessarily a good description of the stocks?\n",
    ">   * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?\n",
    "> 6. We're ready to calculate TF-IDF. Do that for the __5 stock of your choice__. \n",
    ">   * List the 10 top TF words for each stock.\n",
    ">  * List the 10 top TF-IDF words for each stock.\n",
    ">   * Are these 10 words more descriptive of the stock? If yes, what is it about IDF that makes the words more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. First, check out the wikipedia page for TF-IDF. Explain in your own words the point of TF-IDF.\n",
    "What does TF stand for?\n",
    "What does IDF stand for?\"\"\"\n",
    "\n",
    "## TF = Term-Frequency\n",
    "## IDF = Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I read that China's response is something like [80 billion](https://www.ft.com/content/deb56f86-6515-11ea-b3f3-fe4680ea68b5) but i also read something about 200 billion but i might mistaken that with the trade deal thing.\n{'something', 'might', 'china', 'response', 'thing', 'mistaken', 'trade', 'deal', 'i', 'billion', 'also', 'read', 'like'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. Tokenize the text of each submission. Create a column tokens in your dataframe containing the tokens. Remember the bullets below for success.\n",
    "- If you dont' know what tokenization means, go back and read Chapter 3 again. The advice to go back and check Chapter 3 is valid for every cleaning step below.\n",
    "- Exclude punctuation.\n",
    "- Exclude URLs\n",
    "- Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    "- Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    "- Set everything to lower case.\n",
    "- Note that none of the above has to be perfect. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some submissions repeat certain words again and again and again, whereas other are very short. For that reason, I decided to use the unique set of words from each submission rather than each word in proportion to how it's actually used. Choices like that are up to you.\"\"\"\n",
    "\n",
    "punctuations = ['!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    # # Remove URL's\n",
    "    # text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    #print(text)\n",
    "\n",
    "    # Tokenize and rermove punctuations\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and tokens containing any symbol\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in stop_words and not any(symbol in token for symbol in punctuations)]\n",
    "    tokenized_text = [token.replace('$', '') for token in tokenized_text]\n",
    "    \n",
    "    #re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tokenized_text, flags=re.MULTILINE)\n",
    "    return set(tokenized_text)\n",
    "\n",
    "\n",
    "string = \"I read that China's response is something like [80 billion](https://www.ft.com/content/deb56f86-6515-11ea-b3f3-fe4680ea68b5) but i also read something about 200 billion but i might mistaken that with the trade deal thing.\"\n",
    "print(string)\n",
    "print(tokenize(string))\n",
    "\n",
    "wsb_subs['tokens'] = wsb_subs.text.apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/augustsemrau/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import validators\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "\n",
    "    for t in nltk.tokenize.word_tokenize(text):\n",
    "        if (not validators.url(t) and t not in stop_words \n",
    "            and not re.match('\\d+(.\\d+)?', t)):\n",
    "            t = t.replace('.', '').strip().lower()\n",
    "            tokens.append(t)\n",
    "\n",
    "    return tokens \n",
    "\n",
    "\n",
    "wsb_subs['tokens'] = wsb_subs['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_subs.to_csv('/home/augustsemrau/drive/6semester/CSS_02467/comsocsci2021/lectures/data/week6/wsb_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                         title  \\\ncreated_utc                                                      \n1586173811                    What is the Fed actually buying?   \n1586173320        I didn’t learn about puts because I was lazy   \n1586173268                                            HOT TAKE   \n1586172639                                     Fuck you Gordon   \n1586171822                                Can’t find a picture   \n...                                                        ...   \n1602007302                            Hurricane Delta (BECN) 🔥   \n1602006818     Made 40k on Nike. Next play? CROCS motherfucker   \n1602006029   Please screenshot the whole timeline, not just...   \n1602005968    What is your price target for Tesla in 40 years?   \n1601822856     White House infected folks will infect Senators   \n\n                                                      selftext  score  \\\ncreated_utc                                                             \n1586173811   Okay, I may actually just be retarded. On my d...      1   \n1586173320   Beginning of the this virus shit, everyone was...      1   \n1586173268   Literally everyone has free time on their hand...      1   \n1586172639   Gordon I believed in you, I can't even begin t...      1   \n1586171822   Someone uploaded a ohoto of the stock market h...      1   \n...                                                        ...    ...   \n1602007302   \\nHurricane Delta is looking like it is going ...      1   \n1602006818   #  1. Introduction\\n\\n[Proof that I'm lucky](h...      1   \n1602006029   I could nut over your retarded failures just f...      1   \n1602005968   I am 26 and currently max out my roth each yea...      1   \n1601822856   They all gonna get the rona, \\n\\nThis means no...      1   \n\n                                                          text tickers  \\\ncreated_utc                                                              \n1586173811   What is the Fed actually buying? Okay, I may a...      []   \n1586173320   I didn’t learn about puts because I was lazy B...      []   \n1586173268   HOT TAKE Literally everyone has free time on t...      []   \n1586172639   Fuck you Gordon Gordon I believed in you, I ca...      []   \n1586171822   Can’t find a picture Someone uploaded a ohoto ...      []   \n...                                                        ...     ...   \n1602007302   Hurricane Delta (BECN) 🔥 \\nHurricane Delta is ...      []   \n1602006818   Made 40k on Nike. Next play? CROCS motherfucke...      []   \n1602006029   Please screenshot the whole timeline, not just...      []   \n1602005968   What is your price target for Tesla in 40 year...      []   \n1601822856   White House infected folks will infect Senator...      []   \n\n                                                        tokens    stock  \ncreated_utc                                                              \n1586173811   {it, thirdly, one, saying, everyone, whole, lo...  [Other]  \n1586173320   {puts, lazy, everyone, get, shares, googled, c...  [Other]  \n1586173268   {free, it, that, autists, cheap, tldr, everyon...  [Other]  \n1586172639   {dildo, disappointed, gordon, shit, morning, a...  [Other]  \n1586171822   {someone, uploaded, anyone, history, picture, ...  [Other]  \n...                                                        ...      ...  \n1602007302   {hurricanes, harvey, one, throughout, hurrican...  [Other]  \n1602006818   {msrp, reasonable, playing, display, here, sal...  [Other]  \n1602006029   {screenshot, hard, retardation, one, whole, pr...  [Other]  \n1602005968   {will, speculative, shares, years, buy, road, ...  [Other]  \n1601822856   {infect, get, bears, rona, https, folks, model...  [Other]  \n\n[82242 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3. Find submissions discussing at least one of the top 15 stocks you identified above. To do so:\n",
    "\n",
    "Create a function that finds the intersection between a list of tokens and your list of top 15 stocks. For example, your function applied to the tokens: \"Here, TSLA, submission, GME\" should return \"TSLA\",\"GME\". (Optional: you can also try to included cases in which the list of tokens contains a company name among your top 15. For example the function applied to \"Here, Gamestop, submission\" could return 'GME'.)\n",
    "Create a new column stock in your DataFrame, containing the output of your function applied to the text column. Values in this column should be lists.\n",
    "Handle cases where one post discusses more than one stock by applying the function explode to the stock column. This will duplicate submissions associated to multiple stocks. After exploding, the values included in the stock column should be strings.\n",
    "Handle cases where none of the selected stocks is discussed by replacing Nan values, for example with \"Other\".\"\"\"\n",
    "\n",
    "def find_top15(tokens):\n",
    "    intersection = [token for token in tokens if token.upper() in top_15_stocks]\n",
    "    if intersection == []:\n",
    "        intersection = ['Other']\n",
    "    return intersection\n",
    "\n",
    "wsb_subs['stock'] = wsb_subs.text.apply(find_top15)\n",
    "wsb_subs.explode('stock')\n",
    "print(wsb_subs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 'night', 'figure', 'keep', 'ride', 'timing', 'atleast', 'right', 'selling', 'my', 'going', 'healthy', 'take', 'i', 'also', 'trying'}, {'lines', 'signal', 'point', 'due', 'get', 'crush', 'calls', 'bands', 'bias', 'line', 'mid', 'pending', 'tight', 'if', 'move', 'coiling', 'correlates', 'moved', 'consolidation', 'rsi', 'afraid', 'pop', 'directly', 'pins', 'like', 'iv', '8', 'bollinger', 'looks', 'showing', 'macd', 'squeeze', 'much', '6', 'tomorrow', 'relations', 'current', 'nov', 'also', 'snap', 'market', 'zero'}, {'net', 'open', 'spike', 'incoming', 'calls', 'buy', '’ll', 'soon', 'yesterday', 'regret', 'yoloing', 'bought', 'actually', '’m', 'ups', 'would', 'sold', 'ton', 'options', 'zm', 'huge', 'profit', 'i', 'earnings', '200'}, {'moon', 'tech', 'contracts', '59c', '🌝', 'popping', 'stocks', 'going', 'chwy', 'betches', '10', 'this', 'get', 'positions', 'sideways'}, {'dies', 'hey', 'wanting', 'puts', '’all', 'fit', 'wait', 'spy', 'week', 'trump', 'got', '13', '’m', 'oct', 'question', 'flop', 'longer', 'place', 'predict', 'coughing', 'fact', 'i', '’ve'}, {'it', 'dollars', 'into', 'everyone', 'put', 'need', 'sees', 'resistance', 'buy', 'millions', 'up', 'price', 'before', 'passed', 'work', 'this', 'today', 'close', 'of', 'guys', 'almost', 'who', 'and', 'to', 'thanks', 'back', 'post', 'i', 'please', 'go', 'market'}, {'amd', 'finally', 'rolling', 'gme', 'transaction', 'cost', 'unload', 'except', 'puts', 'side', 'want', 'incentivized', 'nkla', 'watch', 'get', 'fraud', 'tired', 'pare', '0', '’ll', 'shit', 'suck', 'keep', 'plan', 'baba', 'fire', 'dog', 'short', 'plntr', 'weather', '’t', 'garbage', 'seeing', 'good', 'plays', 'fuck', 'dkng', 'happy', 'worth', 'bets', 'price', 'till', 'insanity', 'logic', 'hell', 'loss', 'vldr', 'third', 'got', 'hydrogen', 'positions', 'close', 'this', 'dumpster', 'closed', '’m', 'done', 'still', '’s', 'know', 'so', 'much', 'win', 'you', 'what', '’re', 'going', 'bearish', 'i', 'institutional', 'money', 'are', 'go', 'defy'}, {'reason', 'im', 'acquired', 'one', 'open', 'puts', 'somebody', 'weeks', 'gopro', 'calls', 'started', 'tier', 'bankrupt', '7', '4', 'looooong', 'couple', 'ago', 'strikes', 'afk', '100', 'soon', 'go', '4m', 'left', 'gone', 'disclaimer', 'volume', 'bear', 'million', 'really', 'writing', 'time', 'moved', 'hypothesis', 'worth', 'gonna', 'crash', 'no', 'think', 'anyways', 'hadnt', 'opened', 'call', 'sus', 'well', 'got', 'found', 'jan', 'today', 'still', 'gay', 'average', 'stock', 'seems', 'almost', 'sold', 'since', 'something', 'april', 'expecting', '21', '16k', 'my', 'fishy', 'gpro', 'interest', 'dd', 'dudes', 'insiders', 'i', 'jumped', 'burned', 'range', 'eventhough', 'tracking'}, {'advice', 'sub', 'get', 'need', 'first', 'dad', '£1000', '£100', 'penny', 'starting', 'see', 'stocks', 'people', 'would', 'using', 'going', 'post', 'just', 'given', 'i', 'go'}, {'', 'extremely', 'initial', 'find', 'facade', 'human', 'held', 'farm', 'care', 'account', 'that', 'collectively', 'spoke', '’re', '“fuck', 'staggering', 'worldwide', 'pltr', 'reddit', 'maintains', 'publishes', 'bears', '2500', 'day', 'wide', 'basement', 'appear', 'despite', 'mission', 'ruse', 'return', 'time', '“all', 'directly', 'purchasing', 'unedited', 'probably', 'so', '“i', 'special', 'know', 'dude', 'followers', 'bulls', 'hype', 'usually', 'now', 'overarching', 'dumber', 'intimate', 'russian', 'intimidating', 'attempts', 'conversation', 'inadvertently', 'sentience', 'headquarters', 'million', 'shambles', 'sources', 'birth', 'sow', 'retains', 'hyln', 'accusation', 'much', 'enough', 'beaten', 'help', 'my', 'communities', 'bots', 'siphoned', 'lines', 'elon', 'one', '2', 'brutalist', 'talk', 'succumbed', 'zoom', 'experienced', 'all', 'across', 'including', 'mods', 'gang', 'going', 'orphanage', 'current', 'purpose', 'confirm', 'certain', 'contracting', 'consulting', 'surface', 'original', 'gewm', 'wsbers', 'spoken', 'deep', 'choose', 'years', 'accordance', 'show', 'control', 'say', 'robinhood', 'prompted', '“and', 'deposit', 'frequent', 'its', 'u', 'bullshit', 'culture', 'roam', 'donations', 'spokesperson', 'print', 'ever', 'get', 'we', 'subsequent', 'lost', 'outward', 'wall', 'particular', 'through', 'stupid', 'price', 'thread', 'substantiated', 'originally', 'stir', 'make', 'legwork', 'neglect', 'place', '”', 'according', 'within', 'wsb', 'journey', 'chance', 'losses', 'russia', 'company', 'target', 'occasionally', 'spends', 'people', 'tickers', 'operation', 'battery', 'zm', 'late', 'services', 'priced', 'how', 'sub', '“pdt', '“gay', 'global', 'first', 'months', 'soon', 'frequency', 'spent', 'blue', '’t', 'kyvyatsingrad', 'decrepit', 'order', 'see', 'power', 'dozens', 'anonymous', 'access', 'events', 'taken', 'money', 'actual', 'moon', 'it', 'meant', 'fake', 'irresponsibility', 'said', 'hundreds', 'describing', 'lit', 'name', 'true', 'shocking', 'removed', 'emerged', 'seems', 'origins', 'on', 'degeneracy', '“consulting', 'option', 'last', 'calls', 'seed', 'end', 'rare', 'platforms', 'promote', 'touch', 'when', 'recently', 'made', 'remarkable', 'even', 'massive', 'activities', 'tits', '’ve', 'bets', '‘leveraged', 'your', 'become', 'remains', 'luckily', 'buy', 'respond', 'vernacular', 'anything', 'online', 'daddy', 'major', 'no', 'quarter', 'almost', 'tsla', 'the', 'what', 'pitted', 'complex', 'interest', 'usd', 'extent', '“our', 'may', 'deposited', 'immediately', 'anyone', 'opportunity', 'started', 'loll', 'rkt', 'far', 'industrial', 'prepared', 'implications', 'community', 'asked', 'hurting', 'political', 'sentient', 'series', 'contact', 'bro', 'anonymity', 'somewhere', 'options', 'in', 'can', '“we', 'unclear', 'fold', 'media', '100', '“stimmy', 'shit', 'coming', 'contracted', 'moscow', 'troll', 'activity', 'rule', 'trading', 'got', 'american', 'buying', 'involving', 'little', 'march', 'continues', 'suspect', 'really', 'im', 'discord', 'background', 'best', 'fuck', 'learned', 'residents', 'like', 'intentional', 'stands', 'bustling', 'welcomed', 'post', 'potatoboinker', '“but', 'just', 'restriction', 'street', 'i', 'bucharest', 'fruitful', 'came', 'poors', 'called', 'servers', 'guess', 'source', 'except', 'denies', 'overt', 'tell', 'deployed', 'story', 'told', 'highly', 'condition', 'shambled', 'confirmed', 'virulent', 'electronic', 'speaking', 'follows', 'went', 'saw', 'high', 'never', 'building', 'stained', 'social', 'sure', 'market', 'mooooon', 'bot', 'daily', 'violating', 'along', 'town', 'trained', 'outside', 'knowledge', 'diverge', 'speculate', 'estimated', 'cover', 'confusion', '’s', 'appears', 'humans', 'firm', 'transcript', 'however', 'to', 'society', 'experts', 'program', 'documents'}, {'puts', 'experiences', 'example', 'things', 'look', 'whatnot', 'calls', 'for', 'buy', 'study', 'analysis', 'end', 'positive', '’t', 'if', 'reverse', 'past', 'like', 'fundamental', 'trading', 'well', 'common', 'conclude', '’m', 'toooon', 'stock', 'much', 'exact', 'technical', 'instead', 'sense', 'losing', 'going', 'just', 'ought', 'opposite', 'i', 'money', 'go'}, {'will', '100', 'claims', 'algorithms', 'wait', 'disclaimer', 'kodak', 'investment', 'tendie', 'apps', 'trading', 'step', 'temporary', 'right', 'that', 'still', 'risk', 'losing', 'indicative', 'milton', 'future', 'act', 'get', 'amount', 'we', 'tired', 'trevor', 'coupon', 'worthless', 'return', 'time', 'bullet', 'easy', 'even', 'guaranteed', 'longer', 'back', 'taking', 'send', 'invested', 'are', 'pesky', 'quickest', 'expire', 'parent', 'hurry', 'keep', 'company', 'created', 'lead', 'involves', 'mind', 'investments', 'guarantee', 'expert', 'the', 'an', 'based', 'way', 'take', 'fluctuate', 'provided', 'services', 'may', 'free', 'tendies', 'performance', 'wholly', 'less', 'necessarily', 'investing', 'providing', 'investor', 'waiting', 'lose', 'value', 'simply', 'satisfaction', 'offer', 'instant', 'long', 'options', 'available'}, {'amd', 'puts', 'put', 'cpu', 'impress', 'look', 'ahead', 'event', 'cause', 'game', 'hi', 'haha', 'thoughts', 'new', 'believe', 'tomorrow', 'fan', 'semi', 'competitors', '10', 'intel', 'trader', 'boy', 'i'}, {'free', 'part', 'advice', 'laptop', 'need', 'without', 'needed', 'consider', 'seeking', 'equipment', 'years', 'pay', '800', 'etc', '1000', 'next', 'november', 'emergency', 'life', 'seasonsed', 'food', 'subreddit', 'forward', 'move', 'job', 'time', 'debt', 'europe', 'paid', 'yolo', 'month', 'start', '5k', 'fund', 'brings', 'that', 'university', 'means', 'true', 'risk', 'living', 'piece', 'guy', 'now', 'phone', 'bcuz', 'my', 'disposable', 'year', 'among', 'options', 'experts', 'fucking', 'every', 'saved', 'i', 'priveleged', 'car', 'old', 'wsb', 'home'}, {'j', 'hard', 'watching', 'plow', 'mothafuckin', 'dipping', 'we', 'continue', 'sick', 'brrrrrrrrrrrrr', 'hims', 'subscription', 'released', 'maintain', 'bf', 'nuggets', 'turn', 'pow', 'romans', 'wife', 'jus', 'mothafuckn', 'ncno', 'board', 'our', '’s', 'minutes', 'hop', '’m', 'blessed', 'afford', 'the', 'machines', 'fed', 'pills', 'late', 'i', 'sauce'}, {'hours', 'anyone', 'rudders', 'required', 'one', 'terminal', 'away', 'ord', '2', '100', 'ahead', 'sam', 'thing', 'beer', 'october', 'if', 'come', 'roi', 'at', 'wants', 'no', 'sex', 'flight', '’s', 'today', 'needing', 'gay', '’m', 'though', 'never', 'dutch', 'turned', '5', 'i', 'wsb'}, {'dollars', 'do', 'nio', 'potential', 'couple', 'months', 'end', 'target', 'price', 'think', 'overkill', 'tesla', 'where', 'give', 'since', 'what', 'reach', 'around', 'year', 'investors', '’ve', 'i'}, {'preformed', 'balance', 'non', 'rocket', 'room', 'podcasting', 'for', 'sheet', 'expansion', 'positive', 'turn', 'nyt', 'subscriber', 'times', 'new', 'the', 'assets', 'is', 'growth', 'selling', 'around', 'york', 'in'}, {'getting', 'reddit', 'autistic', 'for', 'twtr', 'spending', 'real', 'retarded', 'yall', 'fuck', 'suspended', 'like', 'retards', 'people', 'green', 'still', 'asses', 'twitter', 'fucking', 'they'}, {'actual', 'free', 'printing', 'hours', 'point', 'shares', 'day', '’all', 'literally', '50', 'lol', 'thing', 'icln', 'rest', 'money', 'pre', 'letting', 'stays', 'know', 'its', '1', 'printer', 'just', '20c', 'portfolio', 'every', 'gains', 'gain', 'i', 'flat', 'market'}, {'what', 'think', 'soon', 'playing', 'bought', 'anyone', 'coming', 'crashed', 'bit', 'guys', 'shares', 'today', 'i', 'earnings', 'goes', 'hopefully', 'dominoes'}, {'guess', 'early', 'get', 'calls', 'specific', 'shit', 'best', 'was', 'see', 'bought', 'award', 'be', 'tesla', 'sure', 'moment', 'sold', 'ton', 'what', 'interested', 'losing', 'way', 'whoever', 'just', 'your', 'mine', 'money', 'i'}, {'moon', 'joe', 'dollars', 'index', 'history', 'energy', 'plan', 'anti', 'icln', 'good', 'if', 'sleepy', '15', 'past', 'etfs', 'wins', 'pump', 'stocks', 'all', 'fund', 'trillion', 'outperformed', 'green', 'american', 'so', 'almost', 'january', 'even', 'tech', '21', 'potus', 'year', 'donnie', 'moons', 'every', 'clean', 'general', 'market'}, {'ordeal', 'free', 'do', 'sub', 'morningstar', 'whole', 'subscription', 'fellow', 'mean', 'find', 'perspective', 'if', 'worth', 'often', 'actually', 'trial', 'guys', '’s', 'wanted', 'know', 'usefeul', 'is', 'help', 'use', 'retard', 'i', 'regarding'}, {'net', 'dollars', 'posting', 'did', 'recommend', '55', 'investment', 'time', 'robinhood', 'currently', 'starting', 'loss', 'see', 'people', 'accumulate', 'much', 'invested', 'i', 'gain', 'money', 'thousands', 'how'}, {'companies', 'future', 'material', 'denominator', 'investing', 'worth', 'wondering', 'recently', 'thoughts', 'common', 'found', 'producing', 'know', 'technologies', 'raw', 'biggest', 'is', 'i', 'tin'}, {'companies', 'future', 'material', 'denominator', 'investing', 'real', 'worth', 'wondering', 'recently', 'thoughts', 'common', 'found', 'producing', 'know', 'technologies', 'biggest', 'is', 'i', 'tin'}, {'fucked', '😭🥜', 'anyone', 'honestly', '🍕', 'pizza', 'get', 'feels', 'bears', 'bad', 'else', '’t', 'fuck', 'like', 'month', 'people', 'profited', 'sign', 'buying', 'dominos', 'economy', 'earnings'}, {'note', 'anyone', '133', 'side', 'looking', 'revenue', '17', 'continue', 'calls', 'peleton', 'subscription', 'introduce', 'up', 'likely', 'service', 'itm', 'obviously', 'does', 'otm', 'anybody', 'like', 'blow', 'recurring', 'form', 'make', '’s', 'earnings', '’m', 'oct', 'holding', 'even', 'something', 'sense', 'pton', 'way', 'also', 'on', 'nov', '97', 'i', 'money', 'run', 'hold', 'flip', 'nls', 'may'}, {'existing', '56', '06', 'capable', 'drughouse', 'commitment', 'research', 'joins', 'industrial', 'rapidly', 'manufacturing', 'protype', 'm', 'advanced', 'previous', 'moderna', 'vaccines', 'projects', 'defense', 'awarded', '2013', 'darpa', 'sniper', 'military', 'fund', 'mobile', 'producing', 'grant', 'the', 'technology', 'agency', 'mrna', 'announced', 'builds', 'complex', 'established', 'leveraging', 'development', 'also', 'assistance', 'agreement', 'therapeutics'}, {'called', 'compared', 'strong', '🌈🐻s', 'boys', 'incoming', 'day', '2nd', 'sight', 'foreclosure', '🌈🐻', 'somehow', 'huddle', 'only', 'soon', 'absurd', 'ath', 'horizon', 'fuck', 'week', 'spy', 'bill', 'miniscule', 'trump', 'trading', 'happens', 'right', 'all', 'trillion', 'previously', 'anticipated', 'stonks', 'targeted', 'even', 'would', 'believe', 'multiples', '336p', 'gang', '341p', 'going', 'corona', 'crisis', 'sometime', '10', '5', 'stay', 'stimulus', 'wave', 'i', 'market'}, {'added', 'q3', 'cost', 'hard', 'predicted', 'x', 'q1', 'income', 'looking', 'get', 'shares', 'investing', 'pay', 'contract', 'buy', 'wrap', 'company', 'works', 'thing', 'head', 'a', 'does', 'once', 'understand', 'expendable', 'full', 'price', 'trading', 'already', 'right', 'easy', 'actually', 'otherwise', 'so', 'but', 'new', 'sorry', 'what', 'question', 'stated', 'thanks', 'around', 'options', 'q2', 'fairly', 'i', 'require', 'confused', 'market'}, {'hurricanes', 'harvey', 'one', 'throughout', 'hurricane', 'rise', 'looking', 'expect', 'storms', 'look', 'makes', 'lots', 'golf', 'cause', 'mexico', 'roofing', 'if', 'week', 'performed', 'delta', 'like', 'damage', 'stocks', 'could', 'well', 'make', 'friday', 'news', 'hit', 'this', '2017', 'supply', 'biggest', 'since', 'sense', 'continues', 'going', 'around', 'take', 'beacon', 'year', 'somewhere', 'i', 'sticks', '🔥'}, {'msrp', 'reasonable', 'playing', 'display', 'here', 'sales', 'expectations', 'lows', 'higher', 'figured', 'jordans', 'find', 'if', 'wants', 'always', 'big', 'right', 'brings', 'that', 'autist', 'recent', 'successful', 'and', 'clearly', 'products', 'introduction', 'pattern', 'costco', 'meanwhile', 'beats', 'reappear', 'trying', 'hey', 'withdraw', 'field', 'compared', 'future', 'bieber', 'lucky', 'bears', 'day', 'anticipating', 'explain', '4', 'sell', 'appear', 'august', 'believers', '55c', 'time', 'shipments', 'probably', 'reliable', 'runup', 'so', 'know', 'hype', 'expecting', 'secure', 'especially', 'disaster', 'general', 'unless', 'correlated', 'with', 'not', 'medical', 'million', 'blew', 'making', 'bought', 'third', 'mind', 'adjust', '’m', 'aspects', 'much', 'enough', 'share', 'help', 'conclusion', 'around', 'depends', 'cannot', 'y', 'earnings', 'divergence', 'asleep', 'dumb', 'lines', 'early', 'one', 'picking', 'spiking', '20th', '2', 'updates', 'attention', 'talk', 'bad', '50', 'spy', 'became', 'paying', 'consumer', 'customer', 'talking', 'damned', 'all', 'period', 'analysts', 'anticipation', 'ubiquity', 'going', 'gotten', 'drops', '10', 'wildly', 'rich', 'sneaker', 'certain', 'dealing', 'fucked', 'dollars', 'structure', 'boys', 'storm', 'bunny', 'unusual', 'say', 'reading', 'renaissance', 'regular', 'nearly', 'but', 'still', 'footwear', 'play', 'midyear', 'record', 'supply', 'rumors', 'perfect', 'motherfucker', 'lulu', 'tons', 'fucking', 'also', 'range', 'similar', 'tightening', 'results', 'alright', 'pretty', 'position', 'get', 'stores', 'wheel', 'pair', 'november', 'measuring', 'nyc', 'lost', 'popped', 'unlikely', 'gravity', 'ath', 'hambone', 'wears', 'wall', 'writing', 'pa', 'second', 'pop', 'price', 'could', 'outperforming', 'grateful', 'lot', 'new', 'generally', 'would', 'peak', 'simple', 'are', 'carried', 'while', 'finally', 'indicator', 'targets', 'implied', 'sentence', 'dead', 'doctors', 'logistical', 'as', 'autistic', 'finding', '11', 'keep', 'company', 'follow', 'october', 'k', 'testimonial', 'drop', 'movement', 'runs', 'stocks', 'people', 'means', 'terrible', 'warning', 'of', 'nike', 'able', 'monarchs', 'bonkers', '5', 'issues', '100k', 'dd', 'every', 'late', 'bake', 'near', 'performance', 'getting', 'nonetheless', 'many', 'thousand', 'somebody', 'word', 'everything', 'first', 'next', 'ahead', 'spent', 'real', 'sellers', 'entire', '’t', 'hitting', 'obviously', 'systems', 'see', 'surety', 'suggests', 'behaving', 'closes', 'industry', 'spring', 'component', 'builds', 'choice', 'nj', 'macro', 'chains', 'highs', 'money', 'regarding', 'available', 'strongly', 'sports', 'it', 'part', 'delivery', 'dipping', 'point', 'nikes', 'momentum', 'equipment', 'spending', 'meh', 'collab', 'paid', 'frothy', 'nke', 'reported', 'true', 'compare', 'better', 'september', 'which', 'nearest', 'obvious', 'please', 'consequence', 'simultaneously', 'non', 'last', 'batches', 'welcome', 'waves', 'seconds', 'crocs', 'smarter', 'release', 'end', 'formations', 'retarded', 'familiar', 'snatch', 'prices', 'pops', 'made', 'moment', 'even', 'is', '1', 'selling', 'sites', 'pton', 'back', 'yes', 'although', 'barely', 'production', 'report', 'date', 'put', 'quicker', 'online', 'positive', 'chicken', 'created', '40k', 'there', 'recovering', 'understands', 'goat', 'quarter', 'missing', 'beat', 'narratives', 'tsla', 'the', 'profits', 'way', 'take', 'satisfied', 'broader', 'thousands', 'may', 'starts', 'immediately', 'releases', 'x', 'climbed', 'started', 'returns', 'potential', 'downplayed', 'etc', 'lose', 'upping', 'crox', 'works', 'likely', 's', 'designing', 'devastated', '40', 'timing', 'times', 'shoes', 'alone', 'water', 'close', 'grabbing', 'premarket', '2020', 'quarantine', 'donated', 'established', 'somewhere', 'settling', 'options', 'in', 'posted', 'absolutely', 'saying', 'sleeping', 'specific', 'shit', 'mispriced', '22', 'action', 'pavement', 'move', 'nerds', 'lately', 'summation', 'read', 'rolled', 'surging', '45', 'two', 'weep', 'patterns', 'well', 'shoe', 'biggest', 'opening', '3', 'resale', 'expected', 'billions', 'sort', 'personally', 'comments', 'really', 'collabs', 'matter', 'nobody', 'aggressively', 'fuck', 'feet', 'like', 'crash', 'buttressed', 'reeeeaaaaal', '1st', 'post', 'year', 'i', 'ah', 'street', 'came', 'limited', 'nuts', 'these', 'predicting', 'emphasis', 'sentiment', 'things', 'stockx', 'room', 'dollar', 'great', '115', 'repeating', 'feel', 'center', 'outrageous', 'hardworking', 'outperform', 'basically', 'work', 'secondhand', 'meme', 'saw', 'high', 'an', 'constantly', 'kfc', 'explaining', 'stress', 'never', 'covid', 'let', 'sure', 'software', 'market', 'queue', 'love', 'strong', 'posting', 'want', 'along', 'mostly', 'june', 'crossovers', 'giant', 'folks', 'breakout', 'everywhere', 'more', 'analyst', 'roof', 'headlines', 'swung', 'anybody', 'deliveries', 'start', 'hospital', 'justin', 'summer', '’s', 'this', 'minutes', 'stock', 'appears', '50c', 'exact', 'sold', 'however', 'spend', 'commodity', 'retail', 'air'}, {'screenshot', 'hard', 'retardation', 'one', 'whole', 'providing', 'bad', 'or', 'timeline', 'nice', 'retarded', 'seeing', 'bust', 'gets', 'loses', 'voluptuous', 'loss', 'could', 'call', 'well', 'means', 'work', '’s', 'thinking', 'losing', 'going', 'turned', 'fine', 'nut', 'gains', 'i', 'ultimate', 'please', 'failures', 'really'}, {'will', 'speculative', 'shares', 'years', 'buy', 'road', 'far', 'max', 'roth', 'target', 'robinhood', 'worth', 'currently', 'price', 'think', 'retirement', 'stocks', 'age', 'probably', 'tesla', 'hit', 'high', 'share', 'what', 'growth', 'year', 'i'}, {'infect', 'get', 'bears', 'rona', 'https', 'folks', 'modelo', 'cure', 'wall', 'house', 'gonna', 'drink', 'infected', 'checkies', 'probably', 'means', 'this', 'gay', 'senators', 'corona', 'take', 'white', 'stimulus', 'they', 'street'}]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Now, we want to find out which words are important for each stock, so we're going to create several large documents, one for each stock. Each document includes all the tokens related to the same stock. We will also have a document including discussions that do not relate to the top 15 stocks.\"\"\"\n",
    "\n",
    "top_15_stocks_docs = top_15_stocks\n",
    "\n",
    "for stock in top_15_stocks.keys():\n",
    "    print(stock)\n",
    "    top_15_stocks[stock] = []\n",
    "        \n",
    "    for _, row in wsb_subs.iterrows():\n",
    "        if stock in row['stock']:\n",
    "            top_15_stocks_docs[stock].append(row['tokens'])\n",
    "            \n",
    "\n",
    "print(top_15_stocks_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within 5 stocks of your choice.\n",
    "Describe similarities and differences between the stocks.\n",
    "Why aren't the TFs not necessarily a good description of the stocks?\n",
    "Next, we calculate IDF for every word.\n",
    "What base logarithm did you use? Is that important?\"\"\"\n",
    "print(top_15_stocks['baba'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"6. We're ready to calculate TF-IDF. Do that for the 5 stock of your choice.\n",
    "List the 10 top TF words for each stock.\n",
    "List the 10 top TF-IDF words for each stock.\n",
    "Are these 10 words more descriptive of the stock? If yes, what is it about IDF that makes the words more informative?\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Exercise 4: The Wordcloud_. It's time to visualize our results!\n",
    "\n",
    "> * Install the [`WordCloud`](https://pypi.org/project/wordcloud/) module. \n",
    "> * Now, create word-cloud for each stock. Feel free to make it as fancy or non-fancy as you like. \n",
    "> * Comment on the results. Are these words to be expected? Is there anything that is surprising? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 5: A Study Project in Computational Social Science._\n",
    "> 1. Read the [Project Assignment](https://github.com/lalessan/comsocsci2021/wiki/Project-Assignment) page, where I explain how to set up a Study Project.\n",
    "> 2. Think of a topic of interest to your would like to study using data downloaded from the Web (Wikipedia, Twitter, Reddit, Facebook, Github, other data sources...), and some of the methods we have seen in this course. \n",
    "> 3. What is the topic? \n",
    "> 4. What is the data? \n",
    "> 5. Write down 3 research questions related to your topic that you would like to investigate.\n",
    "> 6. Put together 1 slide including the answers to points 3,4,5.\n",
    "\n",
    "__Important__: This will be by no means the final choice for your Project Assignment. All I want is for you guys to start thinking about it."
   ]
  },
  {
   "source": [
    "1. Done\n",
    "2. Topic for investigation: Are people reading more or less? What if we include news, reddit posts, etc.?\n",
    "3. ngagement.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}